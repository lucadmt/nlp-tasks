{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Automatic Summarization\n",
    "\n",
    "Lo scopo, è di creare automaticamente un riassunto dei documenti proposti in `./data`, e valutarne i risultati (BLEU o ROUGE)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Algoritmo semplice\n",
    "1. Individuare il topic del testo da riassumere (può essere individuato tramite vettori NASARI)\n",
    "2. Crea il contesto raccogliendo i vettori dei termini (si può ripetere ma non c'è uno stopping criterion specificato. Potrebbe essere fatto solo per il primo paragrafo)\n",
    "3. Ritieni i paragrafi in cui le frasi contengono i termini più salienti, basandosi sul WeightedOverlap"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Problemi da affrontare\n",
    "* Content Selection: Quale informazione selezionare, e a che granularità? (si assumono frasi)\n",
    "* \\[Opzionale\\]: Come riordinare e strutturare l'informazione estratta?\n",
    "* \\[Opzionale\\]: Come si ri-pulisce il testo per migliorarne la coerenza?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Se è necessario, scaricare dd-nasari.txt.gz (600Mb) con il seguente snippet:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#import wget\n",
    "#file = wget.download(\"http://www.di.unito.it/~radicion/tmp2del/TLN_180430/dd-nasari.txt.tgz\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import nltk\n",
    "import math\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from common import utils as utils\n",
    "from IPython.display import display\n",
    "from common import prettyprint as pp\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "debug = True\n",
    "def debug_trace(phrase):\n",
    "    if debug:\n",
    "        print(phrase)\n",
    "        print()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Sumy imports\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer as Summarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Prima di tutto, parsifichiamo i contenuti del sottoinsieme di nasari"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "nasari_exp = open('./data/dd-nasari.txt', 'r').read()\n",
    "nasari = [tuple(line.split(';')[1:]) for line in nasari_exp.split('\\n')]\n",
    "nasari = dict([(i[0], list(i[1:])) for i in nasari if i])"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "La seguente funzione restituisce il 'contesto' (una lista di vettori nasari), partendo da una lista di parole in input (può prendere anche solo una parola in input). Il parametro `include_weights`, specifica se includere o meno i pesi delle dimensioni di nasari (falso di default)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def context_from_words(wordlist, include_weights=False):\n",
    "    if type(wordlist) == str:\n",
    "        wordlist = [wordlist]\n",
    "    if include_weights:\n",
    "        return utils.flatten(\n",
    "            [nasari[word.capitalize()] for word in wordlist if word.capitalize() in nasari.keys()]\n",
    "        )\n",
    "    else:\n",
    "        return utils.flatten(\n",
    "            [map(lambda i: i.split('_')[0], nasari[word.capitalize()]) for word in wordlist if word.capitalize() in nasari.keys()]\n",
    "        )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "La seguente funzione, prende in input dei vettori nasari ed un limite alla ricerca di ulteriore contesto. Procede a ricorrere su tutti i termini dei vettori, catturandoli via via per creare il contesto `limit` volte."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def get_recursive_context(starting_vectors, limit=2):\n",
    "    vectors = starting_vectors\n",
    "    result = []\n",
    "    wordset = set(vectors)\n",
    "\n",
    "    # terms with a lesser rank are _already_ less valuable.\n",
    "    max_weight = len(nasari[list(nasari.keys())[0]])\n",
    "\n",
    "    # since the parsed nasari it's a dict {term: [terms]};\n",
    "    # here we have only [terms]\n",
    "    for iteration in range(1, limit):\n",
    "        for dimension in vectors:\n",
    "            if dimension != []:\n",
    "                # Since dimensions are sorted already, start from the maximum importance (the total dimensions)\n",
    "                # Subtract the vector's position (plus one to account for the start at 0)\n",
    "                # Modulus max_weight since they aren't separated in any way.\n",
    "                result.append((dimension, (max_weight-(vectors.index(dimension) % max_weight)+1)/iteration))\n",
    "        new_items = utils.flatten([context_from_words(dim) for dim in vectors])\n",
    "        vectors.clear()\n",
    "        vectors += new_items\n",
    "    temp = sorted(list(set(result)), key=lambda i: i[1], reverse=True)\n",
    "    \n",
    "    clean_dict = {}\n",
    "    # Remove dupes with lesser score\n",
    "    for i in temp:\n",
    "        if i[0] not in clean_dict.keys():\n",
    "            clean_dict[i[0]] = i[1]\n",
    "    return clean_dict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "source": [
    "La seguente funzione parte da una stringa e ne restituisce una versione senza punteggiatura e lemmatizzata, in forma di stringa."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def get_lemmas(text):\n",
    "    punctuation = list(string.punctuation)\n",
    "    punctuation += ['’', '–', '‘', '“', '”', '©']\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return [nltk.stem.WordNetLemmatizer().lemmatize(token.lower()) for token in tokens if (token.lower() not in punctuation)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Piccola funzione per prendere direttamente il tfidf\n",
    "def get_tfidf(word, phrase_idx, matrix):\n",
    "    return matrix.iloc[phrase_idx][word]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Per dare una stima corretta in fase di valutazione è essenziale che gli articoli che vengono riassunti siano presi dagli stessi dati. In fase di valutazione uso direttamente il parser di sumy per ottenere un riassunto 'gold standard'. Dal momento che l'articolo potrebbe essere stato aggiornato e/o contenere ulteriori frasi non presenti nel testo proposto, sfrutto lo stesso parser solo per ottenere i dati di partenza, in modo da non ottenere differenze dovute a questo in fase di valutazione."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def get_url_from_article(path):\n",
    "    f = open(path, 'r')\n",
    "    contents = f.read()\n",
    "    f.close\n",
    "    return contents.split('\\n')[0].replace('# ', '')\n",
    "\n",
    "def contents_from_url(url):\n",
    "    result = []\n",
    "    parser = HtmlParser.from_url(url, Tokenizer(\"english\"))\n",
    "    for paragraph in parser.document.paragraphs:\n",
    "        result.append([str(sent) for sent in paragraph.sentences])\n",
    "        #for sentence in paragraph.sentences:\n",
    "            #print(sentence)\n",
    "    return result"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Anzitutto, per processare l'articolo, lo suddivido in paragrafi, che saranno a loro volta suddivisi in frasi dal `sent_tokenizer` di nltk."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "article_data = open('./data/Andy-Warhol.txt').read()\n",
    "# Use nltk to tokenize phrases from the article\n",
    "article_paragraphs = article_data.split('\\n\\n')\n",
    "article_phrases = [nltk.sent_tokenize(paragraph) for paragraph in article_paragraphs]\n",
    "phraselist = utils.flatten(article_phrases)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Calcolo il Tf.Idf per ogni parola, per ogni frase, l'ho preferito perché è una statistica più robusta della frequenza."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer=get_lemmas, stop_words=stopwords.words('english'))\n",
    "vectors = vectorizer.fit_transform(utils.flatten(article_phrases))\n",
    "tfidfm = pd.DataFrame(vectors.todense().tolist(), columns=vectorizer.get_feature_names())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Position-based\n",
    "Uso la prima euristica proposta e prendo la prima frase nel paragrafo, per ogni paragrafo (**indipendentemente dal suo score**). Per quanto riguarda l'ultima frase, la si valuta come tutte le altre, prima di inserirla (7% non è una buona chance)\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Topic Signature\n",
    "Converto il topic in un modello bag-of-words, rimuovo le parole 'irrilevanti', quello che rimane è la 'topic signature'.\n",
    "\n",
    "Per capire quali sono le parole rilevanti, calcolo la media delle frequenze, e la uso come soglia per le stop-words (questo perché osservando la distribuzione delle parole nei testi proposti, la maggioranza delle parole sono piuttosto irrilevanti (espresse solo una volta))."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "topic_freqs = utils.word_frequencies(\n",
    "    list(filter(lambda i: i not in stopwords.words('english'),\n",
    "        utils.flatten([get_lemmas(phrase) for phrase in utils.flatten(article_phrases)])\n",
    "    ))\n",
    ")\n",
    "stopword_treshold = math.ceil(np.mean(list(zip(*topic_freqs))[1]))\n",
    "debug_trace(f\"{pp.info('Removing words less frequent than: ')}: {stopword_treshold}\")\n",
    "topic_signature = list(filter(lambda i: i[1] > stopword_treshold, topic_freqs))\n",
    "debug_trace(f\"{pp.info('Topic Signature: ')}: {topic_signature}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Contesto\n",
    "Per creare il contesto sfrutto le parole della topic signature. Dopo aver fatto la tokenizzazione della frase (`utils.preprocess_phrase_nostem`), ripulisco il testo di queste prime frasi da eventuali segni di punteggiatura (per evitare che finiscano nel contesto). Quindi uso NASARI per cercare tutte le parole correlate a quella proposta, a causa delle dimensioni osservate, questo viene ripetuto solo due volte."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "keywords = context_from_words(list(zip(*topic_signature))[0])\n",
    "context = get_recursive_context(keywords)\n",
    "debug_trace(f\"{pp.info('Augmented Context')}: {context}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "source": [
    "Nel contesto estratto, ci sono alcune stopwords, quindi le si filtra."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "topic_words = list(zip(*topic_signature))[0]\n",
    "context_words = [context_word for context_word in list(context.keys()) if context_word not in stopwords.words('english')]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "La prossima funzione definisce una misura di salienza. L'idea è di sommare i tf/idf di ogni parola nell'overlap lessicale. Dando punti bonus (`len(topic_overlap)`) per le frasi che contengono parole della topic signature, e per le parole contestuali (`len(context_overlap) * .5`) ma riducendone il peso di metà (dal momento che sono prese da nasari invece che direttamente dal testo)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def saliency(phrase, phrase_list, tfidfm, topic_words, context_words):\n",
    "    phrase_lemmas = get_lemmas(phrase)\n",
    "    phrase_idx = phrase_list.index(phrase)\n",
    "    topic_overlap = list(set(phrase_lemmas).intersection(set(topic_words)))\n",
    "    topic_score = sum([get_tfidf(word, phrase_idx, tfidfm) for word in topic_overlap])\n",
    "    context_overlap = list(set(phrase_lemmas).intersection(set(context_words)))\n",
    "    context_score = sum([get_tfidf(word, phrase_idx, tfidfm) for word in context_overlap])\n",
    "    return topic_score + len(topic_overlap) + context_score + len(context_overlap) * .5\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "source": [
    "Sfruttando la misura appena definita, ho mappato le frasi al loro score e riordinate per salienza decrescente."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "saliency_scores = list(map(lambda phrase: saliency(phrase,phraselist, tfidfm, topic_words, context_words), phraselist))\n",
    "weighted_phrases = sorted(list(zip(phraselist, saliency_scores)), key=lambda i: i[1], reverse=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Prendo come soglia del valore di salienza accettabile, il peso dell'ultimo paragrafo utile prima di sforare la soglia di compressione."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "compression = .3\n",
    "new_len = math.floor(len(phraselist) * (1-compression))\n",
    "phrase_treshold = weighted_phrases[new_len-1][1]\n",
    "\n",
    "result = []\n",
    "for paragraph in article_phrases:\n",
    "    for phrase in paragraph:\n",
    "        # Heuristic #1: If it's the first phrase of the paragraph, take it.\n",
    "        if paragraph.index(phrase) == 0:\n",
    "            result.append(phrase)\n",
    "        # Else, check its saliency\n",
    "        elif saliency(phrase, phraselist, tfidfm, topic_words, context_words) > phrase_treshold:\n",
    "            result.append(phrase)\n",
    "debug_trace(result)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['# https://www.independent.co.uk/arts-entertainment/art/features/andy-warhol-tate-modern-pop-art-elton-john-marilyn-monroe-elvis-presley-a9375021.html', '\\nAndy Warhol: Why the great Pop artist thought ‘Trump is sort of cheap’', 'He anticipated celebrity culture and social media, thought artists should do more than just hold a paintbrush, and wound up John Lennon.', 'As a new Tate exhibition opens, Alastair Smart shows how far the most important artist of the modern age was ahead of his time.', 'uring last year’s Super Bowl, 100 million US viewers were treated to a most unexpected sight in one of the commercial breaks.', 'It was Andy Warhol doing nothing more than taking bites out of a Burger King Whopper – and adding the occasional bit of ketchup – for 45 seconds.', 'There was no music, no punchline, just a little, light rustling of the burger’s wrapper – in a slowly unfolding scene that culminated with the hashtag #EatLikeAndy.', 'It was about as far removed as one could imagine from the big-budget ads traditionally shown during the Super Bowl.', 'Reaction on social media was swift, widespread and mostly containing the word “bizarre”.', 'What financial sense did it make also – given that advertising slots at the Super Bowl are the most expensive in the world, costing $175,000 a second?', 'The answer is that the footage showed a great American artist eating a great American food in the context of a great American occasion.', 'Burger King said the advert was a celebration of the country itself.', '“If that clip proves anything, it’s that Andy is still very much with us,” says Gregor Muir, curator of a major Warhol show that opens at Tate Modern this month.', '“He’s a timeless figure, and our aim with the exhibition is to show that.”', 'After a decade working as a commercial illustrator on Madison Avenue, Warhol began his artistic career at the turn of the 1960s.', 'He sprung to fame as a leader of the Pop Art movement, which rejected high-cultural tradition and depicted everyday subject matter instead: in Warhol’s case, Coke bottles, Brillo boxes and Campbell’s Soup cans.', 'Soon he was silk-screening images of Marilyn Monroe and Elvis Presley too.', 'Andrej and Julia Warhola hailed from a tiny village called Mikova on the edge of the Carpathian Mountains, in what today is Slovakia (but in their day was part of the Austro-Hungarian empire).', 'In search of a better life, they moved to Pittsburgh after the First World War, where Andrej worked in construction and where Andy – Andrew Warhola – was born in 1928.', 'At the start of his career, Andy chose to anglicise his surname, by dropping the “a” at the end.', 'Whenever asked about his ethnic roots, he declared, “I always feel American: 100 per cent.” (Interestingly, Warhol and Donald Trump did actually cross paths – at a New York party in 1981.', 'The latter commissioned the former to create a silkscreen portrait of Trump Tower but turned the finished work down, and refused to pay a cent, when he didn’t like the colour scheme.', 'The Tate exhibition sets out to show how large an influence Warhol has had on artists after him, especially in the way he embraced different media and means of distribution.', 'He also founded Interview, a magazine in which celebrities from Cher to Michael Jackson were interviewed – and which ran for 50 years before its demise in 2018.', 'In short, he believed an artist could, and should, do more than just hold a paintbrush.', 'An interdisciplinary approach to art is today taken as standard – and that’s in no small part thanks to the example set by Warhol’s adventures decades ago.', 'His influence extends far beyond the art world, though.', '“Warhol both predates and predicts the society we live in,” says Muir.', 'On show at Tate Modern will be 25 paintings from a little-known series of paintings from 1975 called Ladies and Gentlemen.', 'Though dead for 33 years, Warhol anticipated the advent of social media.', 'Take his Screen Tests, for example, 472 short films in which visitors to his studio were placed on a chair in front of a video camera and asked to “perform” solo for three minutes to camera – very much like the YouTube vloggers of today.', 'Then there’s his 1975 book The Philosophy of Andy Warhol, usually described as a disjointed autobiography but better understood as a proto-Twitter account, given its author’s propensity for including both the most mundane of details (such as his love of jam on toast) and witty one-liners (such as “buying is more American than thinking”).', 'He also carried a compact Minox camera with him wherever he went, taking more than 100,000 photographs of friends, food, building facades, shop signs and the like – anticipating the Instagram feeds of our image-saturated age today.', '“I just look at pictures.” (In his recently published autobiography, Me, Elton John tells the hilarious story of a time he and John Lennon had been taking piles of cocaine in a New York hotel room late into the night, when Warhol knocked at the door.', 'These individual episodes in Warhol’s career show his innate grasp of communications technology; the speed at which it was changing; and the direction in which it was heading.', 'He was aware of the inexorable rise of television, particularly with the proliferation of cable channels such as CNN, ESPN, Nickelodeon and MTV in the late-1970s and early-1980s.', 'Warhol declared at the time that television “was the medium [he’d] most now like to shine in”, and he actually presented five episodes of an MTV talk show called Andy Warhol’s Fifteen Minutes – a job cut short by his death, aged 58, from complications after a gall bladder operation.', 'The show’s title referred to his most famous quote: “In the future, everyone will be world-famous for 15 minutes.”', 'With these words, Warhol predicted the coming both of reality TV stars and social media influencers – as the platforms to achieve quick-fire celebrity grew exponentially.', '“It’s often said Warhol was the most important artist of the second half of the 20th century,” Muir says.', '“But one might well argue he’s the most important artist of the early 21st century too.” In a consumerist society, he realised that celebrities are every bit as ubiquitous and disposable as soup cans, Coke bottles and Burger King Whoppers.']\n\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "La prossima funzione riassume quanto fatto fin'ora, solo mettendo tutto assieme per restituire una lista di stringhe in output"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "def summarize(filename, compression=.3):\n",
    "    # get the articles and parse them\n",
    "    contents = contents_from_url(get_url_from_article(filename))\n",
    "    # convert [[phrase]] -> [phrase]\n",
    "    phraselist = utils.flatten(contents)\n",
    "    # Compute tf.idf matrix\n",
    "    vectorizer = TfidfVectorizer(tokenizer=get_lemmas, stop_words=stopwords.words('english'))\n",
    "    vectors = vectorizer.fit_transform(phraselist)\n",
    "    tfidfm = pd.DataFrame(vectors.todense().tolist(), columns=vectorizer.get_feature_names())\n",
    "    # Compute topic signature\n",
    "    topic_freqs = utils.word_frequencies(\n",
    "    list(filter(lambda i: i not in stopwords.words('english'),\n",
    "        utils.flatten([get_lemmas(phrase) for phrase in phraselist])\n",
    "        ))\n",
    "    )\n",
    "    stopword_treshold = math.ceil(np.mean(list(zip(*topic_freqs))[1]))\n",
    "    debug_trace(f\"{pp.info('Removing words less frequent than: ')}: {stopword_treshold}\")\n",
    "    topic_signature = list(filter(lambda i: i[1] > stopword_treshold, topic_freqs))\n",
    "    debug_trace(f\"{pp.info('Topic Signature: ')}: {topic_signature}\")\n",
    "    # Compute Augmented Context\n",
    "    keywords = context_from_words(list(zip(*topic_signature))[0])\n",
    "    context = get_recursive_context(keywords)\n",
    "    topic_words = list(zip(*topic_signature))[0]\n",
    "    context_words = [context_word for context_word in list(context.keys()) if context_word not in stopwords.words('english')]\n",
    "    debug_trace(f\"{pp.info('Augmented Context: ')}: {context}\")\n",
    "    saliency_scores = list(map(lambda phrase: saliency(phrase, phraselist, tfidfm, topic_words, context_words), phraselist))\n",
    "    weighted_phrases = sorted(list(zip(phraselist, saliency_scores)), key=lambda i: i[1], reverse=True)\n",
    "    new_len = math.floor(len(phraselist) * (1-compression))\n",
    "    phrase_treshold = weighted_phrases[new_len-1][1]\n",
    "    result = []\n",
    "    for paragraph in contents:\n",
    "        for phrase in paragraph:\n",
    "            # Heuristic #1: If it's the first phrase of the paragraph, take it.\n",
    "            if paragraph.index(phrase) == 0:\n",
    "                result.append(phrase)\n",
    "            # Else, check its saliency\n",
    "            elif saliency(phrase, phraselist, tfidfm, topic_words, context_words) > phrase_treshold:\n",
    "                result.append(phrase)\n",
    "    return result\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Valutazione"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "def get_gold_summary(filename, compression=.3):\n",
    "    url = open(filename).read().split('\\n')[0].replace('# ', '')\n",
    "    debug_trace(f\"{pp.info('Extracted URL')}: {url}\")\n",
    "    LANGUAGE = \"english\"\n",
    "    parser = HtmlParser.from_url(url, Tokenizer(LANGUAGE))\n",
    "    SENTENCES_COUNT = math.floor(len(parser.document.sentences) * (1-compression))\n",
    "    stemmer = Stemmer(LANGUAGE)\n",
    "    summarizer = Summarizer(stemmer)\n",
    "    summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "    return [str(sentence) for sentence in summarizer(parser.document, SENTENCES_COUNT)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "def rouge(summary, gold_standard):\n",
    "    return len(set(summary).intersection(set(gold_standard)))/len(summary)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "def round_evaluate(files):\n",
    "    results = dict()\n",
    "    for compression in [.1, .2, .3, .4, .5, .6, .7, .8, .9]:\n",
    "        for article in files:\n",
    "            original = summarize(article, compression=compression)\n",
    "            gold = get_gold_summary(article, compression=compression)\n",
    "            score = rouge(original, gold)\n",
    "            results[f\"{article}@{compression}\"] = score\n",
    "            if score > .5:\n",
    "                print(f\"{pp.info(article)}@{pp.info(compression)}: {pp.success(score)}\")\n",
    "            else:\n",
    "                print(f\"{pp.info(article)}@{pp.info(compression)}: {pp.fail(score)}\")\n",
    "        print()\n",
    "    return results\n",
    "            \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Risultati"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "debug=False\n",
    "results = round_evaluate(['./data/Andy-Warhol.txt', './data/Ebola-virus-disease.txt', './data/Life-indoors.txt', './data/Napoleon-wiki.txt', './data/Trump-wall.txt'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[96m./data/Andy-Warhol.txt\u001b[0m@\u001b[96m0.1\u001b[0m: \u001b[92m0.86\u001b[0m\n",
      "\u001b[96m./data/Ebola-virus-disease.txt\u001b[0m@\u001b[96m0.1\u001b[0m: \u001b[92m0.8276062900407688\u001b[0m\n",
      "\u001b[96m./data/Life-indoors.txt\u001b[0m@\u001b[96m0.1\u001b[0m: \u001b[92m0.9615384615384616\u001b[0m\n",
      "\u001b[96m./data/Napoleon-wiki.txt\u001b[0m@\u001b[96m0.1\u001b[0m: \u001b[92m0.933873986275733\u001b[0m\n",
      "\u001b[96m./data/Trump-wall.txt\u001b[0m@\u001b[96m0.1\u001b[0m: \u001b[92m0.7564526803441429\u001b[0m\n",
      "\n",
      "\u001b[96m./data/Andy-Warhol.txt\u001b[0m@\u001b[96m0.2\u001b[0m: \u001b[92m0.782608695652174\u001b[0m\n",
      "\u001b[96m./data/Ebola-virus-disease.txt\u001b[0m@\u001b[96m0.2\u001b[0m: \u001b[92m0.7616822429906542\u001b[0m\n",
      "\u001b[96m./data/Life-indoors.txt\u001b[0m@\u001b[96m0.2\u001b[0m: \u001b[92m0.8461538461538461\u001b[0m\n",
      "\u001b[96m./data/Napoleon-wiki.txt\u001b[0m@\u001b[96m0.2\u001b[0m: \u001b[92m0.8864864864864865\u001b[0m\n",
      "\u001b[96m./data/Trump-wall.txt\u001b[0m@\u001b[96m0.2\u001b[0m: \u001b[92m0.7605947955390334\u001b[0m\n",
      "\n",
      "\u001b[96m./data/Andy-Warhol.txt\u001b[0m@\u001b[96m0.3\u001b[0m: \u001b[92m0.7142857142857143\u001b[0m\n",
      "\u001b[96m./data/Ebola-virus-disease.txt\u001b[0m@\u001b[96m0.3\u001b[0m: \u001b[92m0.7649700598802395\u001b[0m\n",
      "\u001b[96m./data/Life-indoors.txt\u001b[0m@\u001b[96m0.3\u001b[0m: \u001b[92m0.782608695652174\u001b[0m\n",
      "\u001b[96m./data/Napoleon-wiki.txt\u001b[0m@\u001b[96m0.3\u001b[0m: \u001b[92m0.8806774441878368\u001b[0m\n",
      "\u001b[96m./data/Trump-wall.txt\u001b[0m@\u001b[96m0.3\u001b[0m: \u001b[92m0.7249575551782682\u001b[0m\n",
      "\n",
      "\u001b[96m./data/Andy-Warhol.txt\u001b[0m@\u001b[96m0.4\u001b[0m: \u001b[92m0.5789473684210527\u001b[0m\n",
      "\u001b[96m./data/Ebola-virus-disease.txt\u001b[0m@\u001b[96m0.4\u001b[0m: \u001b[92m0.7439236111111112\u001b[0m\n",
      "\u001b[96m./data/Life-indoors.txt\u001b[0m@\u001b[96m0.4\u001b[0m: \u001b[92m0.6666666666666666\u001b[0m\n",
      "\u001b[96m./data/Napoleon-wiki.txt\u001b[0m@\u001b[96m0.4\u001b[0m: \u001b[92m0.8521816562778273\u001b[0m\n",
      "\u001b[96m./data/Trump-wall.txt\u001b[0m@\u001b[96m0.4\u001b[0m: \u001b[92m0.7118811881188118\u001b[0m\n",
      "\n",
      "\u001b[96m./data/Andy-Warhol.txt\u001b[0m@\u001b[96m0.5\u001b[0m: \u001b[92m0.5142857142857142\u001b[0m\n",
      "\u001b[96m./data/Ebola-virus-disease.txt\u001b[0m@\u001b[96m0.5\u001b[0m: \u001b[92m0.7160621761658031\u001b[0m\n",
      "\u001b[96m./data/Life-indoors.txt\u001b[0m@\u001b[96m0.5\u001b[0m: \u001b[92m0.5263157894736842\u001b[0m\n",
      "\u001b[96m./data/Napoleon-wiki.txt\u001b[0m@\u001b[96m0.5\u001b[0m: \u001b[92m0.8185654008438819\u001b[0m\n",
      "\u001b[96m./data/Trump-wall.txt\u001b[0m@\u001b[96m0.5\u001b[0m: \u001b[92m0.6860189573459715\u001b[0m\n",
      "\n",
      "\u001b[96m./data/Andy-Warhol.txt\u001b[0m@\u001b[96m0.6\u001b[0m: \u001b[91m0.375\u001b[0m\n",
      "\u001b[96m./data/Ebola-virus-disease.txt\u001b[0m@\u001b[96m0.6\u001b[0m: \u001b[92m0.7038461538461539\u001b[0m\n",
      "\u001b[96m./data/Life-indoors.txt\u001b[0m@\u001b[96m0.6\u001b[0m: \u001b[91m0.47058823529411764\u001b[0m\n",
      "\u001b[96m./data/Napoleon-wiki.txt\u001b[0m@\u001b[96m0.6\u001b[0m: \u001b[92m0.7992277992277992\u001b[0m\n",
      "\u001b[96m./data/Trump-wall.txt\u001b[0m@\u001b[96m0.6\u001b[0m: \u001b[92m0.7559171597633136\u001b[0m\n",
      "\n",
      "\u001b[96m./data/Andy-Warhol.txt\u001b[0m@\u001b[96m0.7\u001b[0m: \u001b[91m0.2903225806451613\u001b[0m\n",
      "\u001b[96m./data/Ebola-virus-disease.txt\u001b[0m@\u001b[96m0.7\u001b[0m: \u001b[92m0.6728499156829679\u001b[0m\n",
      "\u001b[96m./data/Life-indoors.txt\u001b[0m@\u001b[96m0.7\u001b[0m: \u001b[91m0.4\u001b[0m\n",
      "\u001b[96m./data/Napoleon-wiki.txt\u001b[0m@\u001b[96m0.7\u001b[0m: \u001b[92m0.6807817589576547\u001b[0m\n",
      "\u001b[96m./data/Trump-wall.txt\u001b[0m@\u001b[96m0.7\u001b[0m: \u001b[92m0.8180039138943248\u001b[0m\n",
      "\n",
      "\u001b[96m./data/Andy-Warhol.txt\u001b[0m@\u001b[96m0.8\u001b[0m: \u001b[91m0.17857142857142858\u001b[0m\n",
      "\u001b[96m./data/Ebola-virus-disease.txt\u001b[0m@\u001b[96m0.8\u001b[0m: \u001b[92m0.624390243902439\u001b[0m\n",
      "\u001b[96m./data/Life-indoors.txt\u001b[0m@\u001b[96m0.8\u001b[0m: \u001b[91m0.21428571428571427\u001b[0m\n",
      "\u001b[96m./data/Napoleon-wiki.txt\u001b[0m@\u001b[96m0.8\u001b[0m: \u001b[92m0.5098901098901099\u001b[0m\n",
      "\u001b[96m./data/Trump-wall.txt\u001b[0m@\u001b[96m0.8\u001b[0m: \u001b[92m0.7110481586402266\u001b[0m\n",
      "\n",
      "\u001b[96m./data/Andy-Warhol.txt\u001b[0m@\u001b[96m0.9\u001b[0m: \u001b[91m0.1111111111111111\u001b[0m\n",
      "\u001b[96m./data/Ebola-virus-disease.txt\u001b[0m@\u001b[96m0.9\u001b[0m: \u001b[91m0.40756302521008403\u001b[0m\n",
      "\u001b[96m./data/Life-indoors.txt\u001b[0m@\u001b[96m0.9\u001b[0m: \u001b[91m0.16666666666666666\u001b[0m\n",
      "\u001b[96m./data/Napoleon-wiki.txt\u001b[0m@\u001b[96m0.9\u001b[0m: \u001b[91m0.30990415335463256\u001b[0m\n",
      "\u001b[96m./data/Trump-wall.txt\u001b[0m@\u001b[96m0.9\u001b[0m: \u001b[91m0.49019607843137253\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "source": [
    "Bisogna arrivare a comprimere il testo del 90% per avere performance in recall molto basse su tutti gli articoli. Bisogna comprimere il testo al 60% per avere una performance molto bassa su 2/5 articoli. Gli articoli sono lunghi (in parole), rispettivamente:\n",
    "\n",
    "`./data/Andy-Warhol.txt`: 1227  \n",
    "`./data/Ebola-virus-disease.txt`: 2106  \n",
    "`./data/Life-indoors.txt`: 567  \n",
    "`./data/Napoleon-wiki.txt`: 1006  \n",
    "`./data/Trump-wall.txt`: 3738  \n",
    "\n",
    "I primi a perdere sufficientemente in recall sono Andy-Warhol e Life-indoors. All'inizio ho pensato che avesse a che fare con la lunghezza originale dell'articolo, se non fosse che l'articolo su Andy Warhol è il doppio più lungo di Life-indoors e il terzo per lunghezza.\n",
    "\n",
    "L'ipotesi che reputo più probabile è che il modo che ho intrapreso per fare il riassunto (l'euristica 'position-based' e i vettori di nasari) è una buon modo per riassumere pagine di Wikipedia. Che ha senso, anche perché nasari è creata a partire da concetti su BabelNet (creata a partire da WordNet e Wikipedia). Mentre invece la performance peggiora molto e molto prima, su articoli di natura generale."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python39664bit04df4e6c9bd54666ad2d5ca24f16c028"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}