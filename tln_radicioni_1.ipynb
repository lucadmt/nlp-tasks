{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a5251d4-cd45-4333-9560-d1b3961a7ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import math\n",
    "import operator\n",
    "import functools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from common import utils\n",
    "from common import prettyprint as pp\n",
    "from nltk.corpus import wordnet as wn\n",
    "from scipy.stats.stats import pearsonr, spearmanr"
   ]
  },
  {
   "source": [
    "La seguente funzione calcola il percorso di massima lunghezza su WordNet, è piuttosto lento, quindi lo calcolo qui, una volta sola"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a724b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def depthMax():\n",
    "    return max(max(len(hyp_path) for hyp_path in ss.hypernym_paths()) for ss in wn.all_synsets())\n",
    "\n",
    "depth_max = depthMax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a87d5494",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = True\n",
    "def debug_trace(phrase):\n",
    "    if debug:\n",
    "        print(phrase)"
   ]
  },
  {
   "source": [
    "Le seguenti sono due funzioni alternative per calcolare il lowest common subsumer. La prima (`lcs_orig`) è quella che ho creato interamente da me. La seconda (`lcs_adapted`), è una specializzazione della prima, in cui ho aggiunto alcuni step che (sembra) fare la controparte di WordNet."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58d6a5ce-2cd0-4e2a-a48b-4cfb806f4f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowest Common Subsumer\n",
    "def lcs_orig(c1, c2):\n",
    "    hypernyms = lambda x: x.hypernyms()\n",
    "    for i in range(0, max([c1.max_depth(), c2.max_depth()])):\n",
    "    \n",
    "        c1_set = set(c1.closure(hypernyms, depth=i))\n",
    "        c2_set = set(c2.closure(hypernyms, depth=i))\n",
    "    \n",
    "        lcs = c1_set.intersection(c2_set)\n",
    "    \n",
    "        if (lcs != set()):\n",
    "            return list(lcs)\n",
    "    return None\n",
    "\n",
    "# Lowest Common Subsumer\n",
    "def lcs_adapted(c1, c2):\n",
    "    hypernyms = lambda x: x.hypernyms()\n",
    "    # IDK, nltk does this too\n",
    "    if c1 == c2:\n",
    "        return [c1]\n",
    "    elif c2 in c1.hypernyms():\n",
    "        return [c2]\n",
    "    elif c1 in c2.hypernyms():\n",
    "        return [c1]\n",
    "    for i in range(0, max([c1.max_depth(), c2.max_depth()])+1):\n",
    "    \n",
    "        c1_set = set(c1.closure(hypernyms, depth=i))\n",
    "        c2_set = set(c2.closure(hypernyms, depth=i))\n",
    "    \n",
    "        lcs = c1_set.intersection(c2_set)\n",
    "    \n",
    "        if (lcs != set()):\n",
    "            return list(lcs)\n",
    "    return None"
   ]
  },
  {
   "source": [
    "Seguono le implementazioni delle funzioni richieste.\n",
    "### Wu-Palmer:\n",
    "$$\\text{cs}(s_1, s_2) = \\frac{2 \\cdot \\text{depth}(LCS)}{\\text{depth}(s_1) + \\text{depth}(s_2)}$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "958a38f7-6aa1-46b1-b7ee-c6ad4c72ac19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wup_similarity(c1, c2, lcsfun=lcs_adapted):\n",
    "    c_lcs = lcsfun(c1, c2)\n",
    "    #print(c_lcs)\n",
    "    if c_lcs != None:\n",
    "        return 2*c_lcs[0].max_depth()/(c1.max_depth() + c2.max_depth())\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "source": [
    "### Shortest Path\n",
    "$$\\text{sim}_{\\text{path}}(s_1, s_2) = 2 \\cdot \\text{depthMax} - \\text{shortest_path_len}(s_1, s_2)$$\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfbd8686-3d8d-4277-a922-49655cab15a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pth_similarity(c1, c2):\n",
    "    shortest_len = c1.shortest_path_distance(c2)\n",
    "    if shortest_len != None:\n",
    "        return (2*depth_max - shortest_len)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "source": [
    "### Leacock-Chodorow\n",
    "$$\\text{sim}_{\\text{LC}}(s_1, s_2) = -\\log \\frac{\\text{shortest_path_len}(s_1, s_2)}{2 \\cdot \\text{depthMax}}$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6658d17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lch_similarity(c1, c2):\n",
    "    shortest_len = c1.shortest_path_distance(c2)\n",
    "    if shortest_len != None:\n",
    "        return -math.log((1+shortest_len)/(1+(2*depth_max)))\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca1a46d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsims = pd.read_csv('./data/WordSim353.csv')"
   ]
  },
  {
   "source": [
    "La seguente funzione calcola la similarità tra due *termini*, massimizzando il valore di una certa funzione. Anzitutto prende i synsets per entrambe. Calcola il risultato, e se fosse maggiore dell'attuale massimo (inizializzato a 0 per essere rimpiazzato) lo userebbe come nuovo massimo. Quindi, dopo aver iterato su tutte le combinazioni di tutti i sensi, restituisce il risultato con score massimo."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cc2c308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim(word1, word2, similarity=wup_similarity):\n",
    "    ss1 = wn.synsets(word1)\n",
    "    ss2 = wn.synsets(word2)\n",
    "    results = list()\n",
    "    cur_max = (word1,word2,0)\n",
    "    for i in utils.gen_combo(ss1, ss2):\n",
    "        result = similarity(i[0], i[1])\n",
    "        if result != None and result > cur_max[2]:\n",
    "            cur_max = (i[0], i[1], result)\n",
    "    return [cur_max]"
   ]
  },
  {
   "source": [
    "Nei prossimi blocchi, raccolgo tutti i risultati, calcolati di volta in volta con funzione di similarità differente, e creo un DataFrame (pandas) contenente i risultati calcolati."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1787d41d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wup_series = []\n",
    "path_series = []\n",
    "lch_series = []\n",
    "\n",
    "for i in range(len(wordsims)):\n",
    "    row = wordsims.iloc[i]\n",
    "\n",
    "    # find all the senses, then sim = max of them\n",
    "    wup_series.append(sim(row.Word1, row.Word2, similarity=wup_similarity))\n",
    "    path_series.append(sim(row.Word1, row.Word2, similarity=pth_similarity))\n",
    "    lch_series.append(sim(row.Word1, row.Word2, similarity=lch_similarity))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4892a248",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata = pd.DataFrame(data={\n",
    "    'Word1': wordsims.Word1,\n",
    "    'Word2': wordsims.Word2,\n",
    "    'Target': wordsims.HumanMean,\n",
    "    'WUP': list(zip(*[i[0] for i in wup_series]))[2],\n",
    "    'PTH': list(zip(*[i[0] for i in path_series]))[2],\n",
    "    'LCH': list(zip(*[i[0] for i in lch_series]))[2],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f6d18ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata.to_csv('./results/term_similarity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            Word1     Word2  Target       WUP  PTH       LCH\n",
       "0            love       sex    6.77  0.909091   39  3.020425\n",
       "1           tiger       cat    7.35  0.962963   39  3.020425\n",
       "2           tiger     tiger   10.00  1.000000   40  3.713572\n",
       "3            book     paper    7.46  0.857143   38  2.614960\n",
       "4        computer  keyboard    7.62  0.800000   37  2.327278\n",
       "..            ...       ...     ...       ...  ...       ...\n",
       "348        shower     flood    6.03  0.533333   36  2.104134\n",
       "349       weather  forecast    8.34  0.000000   27  1.074515\n",
       "350      disaster      area    6.25  0.428571   32  1.516347\n",
       "351      governor    office    6.34  0.470588   31  1.410987\n",
       "352  architecture   century    3.78  0.181818   31  1.410987\n",
       "\n",
       "[353 rows x 6 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word1</th>\n      <th>Word2</th>\n      <th>Target</th>\n      <th>WUP</th>\n      <th>PTH</th>\n      <th>LCH</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>love</td>\n      <td>sex</td>\n      <td>6.77</td>\n      <td>0.909091</td>\n      <td>39</td>\n      <td>3.020425</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>tiger</td>\n      <td>cat</td>\n      <td>7.35</td>\n      <td>0.962963</td>\n      <td>39</td>\n      <td>3.020425</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>tiger</td>\n      <td>tiger</td>\n      <td>10.00</td>\n      <td>1.000000</td>\n      <td>40</td>\n      <td>3.713572</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>book</td>\n      <td>paper</td>\n      <td>7.46</td>\n      <td>0.857143</td>\n      <td>38</td>\n      <td>2.614960</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>computer</td>\n      <td>keyboard</td>\n      <td>7.62</td>\n      <td>0.800000</td>\n      <td>37</td>\n      <td>2.327278</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>348</th>\n      <td>shower</td>\n      <td>flood</td>\n      <td>6.03</td>\n      <td>0.533333</td>\n      <td>36</td>\n      <td>2.104134</td>\n    </tr>\n    <tr>\n      <th>349</th>\n      <td>weather</td>\n      <td>forecast</td>\n      <td>8.34</td>\n      <td>0.000000</td>\n      <td>27</td>\n      <td>1.074515</td>\n    </tr>\n    <tr>\n      <th>350</th>\n      <td>disaster</td>\n      <td>area</td>\n      <td>6.25</td>\n      <td>0.428571</td>\n      <td>32</td>\n      <td>1.516347</td>\n    </tr>\n    <tr>\n      <th>351</th>\n      <td>governor</td>\n      <td>office</td>\n      <td>6.34</td>\n      <td>0.470588</td>\n      <td>31</td>\n      <td>1.410987</td>\n    </tr>\n    <tr>\n      <th>352</th>\n      <td>architecture</td>\n      <td>century</td>\n      <td>3.78</td>\n      <td>0.181818</td>\n      <td>31</td>\n      <td>1.410987</td>\n    </tr>\n  </tbody>\n</table>\n<p>353 rows × 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "newdata"
   ]
  },
  {
   "source": [
    "Nei prossimi blocchi calcolo la correlazione tra i valori target e i valori calcolati, sfruttando le funzioni di scipy."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Pearson Correlation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[92m\u001b[1mWu-Palmer\u001b[0m\u001b[0m: 0.2858796939892433\n\u001b[92m\u001b[1mPath Similarity\u001b[0m\u001b[0m: 0.16653216769600956\n\u001b[92m\u001b[1mLeacock\u001b[0m\u001b[0m: 0.3143602416139906\n"
     ]
    }
   ],
   "source": [
    "print(f\"{pp.success(pp.bold('Wu-Palmer'))}: {pearsonr(newdata.WUP, newdata.Target)[0]}\")\n",
    "print(f\"{pp.success(pp.bold('Path Similarity'))}: {pearsonr(newdata.PTH, newdata.Target)[0]}\")\n",
    "print(f\"{pp.success(pp.bold('Leacock'))}: {pearsonr(newdata.LCH, newdata.Target)[0]}\")"
   ]
  },
  {
   "source": [
    "### Spearman Correlation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[92m\u001b[1mWu-Palmer\u001b[0m\u001b[0m: 0.32106636742234207\n\u001b[92m\u001b[1mPath Similarity\u001b[0m\u001b[0m: 0.2895253335747299\n\u001b[92m\u001b[1mLeacock\u001b[0m\u001b[0m: 0.2895253335747299\n"
     ]
    }
   ],
   "source": [
    "print(f\"{pp.success(pp.bold('Wu-Palmer'))}: {spearmanr(newdata.WUP, newdata.Target)[0]}\")\n",
    "print(f\"{pp.success(pp.bold('Path Similarity'))}: {spearmanr(newdata.PTH, newdata.Target)[0]}\")\n",
    "print(f\"{pp.success(pp.bold('Leacock'))}: {spearmanr(newdata.LCH, newdata.Target)[0]}\")"
   ]
  },
  {
   "source": [
    "La correlazione è positiva in entrambi i casi: Quando il valore delle funzioni descritte sopra sale, così fa anche il target."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk.wsd\n",
    "from nltk.corpus import semcor as sc"
   ]
  },
  {
   "source": [
    "# Part 2: Word Sense Disambiguation\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Lesk Algorithm Implementation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "La prossima funzione calcola la sovrapposizione lessicale tra signature e context. Ho sperimentato la rilevanza del togliere dai rispettivi insiemi di parole (contesto e signature) il termine a cui si riferisce la signature, per controllare se avesse un impatto sulle valutazioni finali o meno (non ce l'ha)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeOverlap(signature, context, word=None):\n",
    "    if len(signature) == 0 or len(context) == 0:\n",
    "        return 0\n",
    "    try:\n",
    "        signature_wordset = set(list(zip(*signature))[0])\n",
    "        context_wordset = set(list(zip(*context))[0])\n",
    "        debug_trace(f\"{pp.info('INFO')}: signature: {signature_wordset}, context: {context_wordset}\")\n",
    "        totalwords = len(context_wordset)\n",
    "    except IndexError:\n",
    "        print(len(context), len(signature))\n",
    "        print(context)\n",
    "        print(signature)\n",
    "    if word != None:\n",
    "        # remove the target word from both wordsets\n",
    "        try:\n",
    "            signature_wordset.remove(word)\n",
    "            context_wordset.remove(word)\n",
    "        except KeyError:\n",
    "            pass\n",
    "    # +1 to avoid zerodivision error\n",
    "    return (len(list(signature_wordset.intersection(context_wordset))) +1)/(totalwords+1)"
   ]
  },
  {
   "source": [
    "La prossima funzione si occupa di creare il contesto dato un synset, sfruttando esempi e definizioni, sia del dato synset, che dei suoi iperonimi diretti e iponimi diretti, il parametro do_stemming specifica se fare lo stemming dei risultati o meno."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_big_disambiguation_ctx_for_sysnet(synset, do_stemming=True):\n",
    "    # Given a synset, returns a Bag-of-Words model for its definition and examples\n",
    "    # as well as those of the direct hypernyms and direct hyponyms\n",
    "    context = synset.definition()\n",
    "    for example in synset.examples():\n",
    "        context += example\n",
    "\n",
    "    for hypernym in synset.hypernyms():\n",
    "        context += hypernym.definition()\n",
    "        for hyp_example in hypernym.examples():\n",
    "            context += hyp_example\n",
    "    \n",
    "    for hyponym in synset.hyponyms():\n",
    "        context += hyponym.definition()\n",
    "        for hypo_example in hyponym.examples():\n",
    "            context += hypo_example\n",
    "    if do_stemming:\n",
    "        return utils.word_frequencies(utils.preprocess_phrase(context))\n",
    "    else:\n",
    "        return utils.word_frequencies(utils.preprocess_phrase_nostem(context))"
   ]
  },
  {
   "source": [
    "I prossimi blocchi contengono l'algoritmo di Lesk e alcune specializzazioni che ho testato, per controllare l'impatto sui risultati. L'implementazione del prossimo blocco è quella standard, senza alcun tipo di modifiche, una banale traduzione dallo pseudocodice."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lesk(word, sentence, overlapf=computeOverlap):\n",
    "    try:\n",
    "        best_sense = wn.synsets(word)[0]\n",
    "    except IndexError:\n",
    "        best_sense = None\n",
    "    max_overlap = 0\n",
    "    context = utils.word_frequencies(utils.preprocess_phrase(sentence))\n",
    "    for sense in wn.synsets(word):\n",
    "        debug_trace(f\"{pp.info('INFO')}: Considering new sense: {sense}({pp.bold(pp.info(sense.definition()))})\")\n",
    "        signature = utils.word_frequencies(utils.preprocess_phrase(sense.definition()))\n",
    "        overlap = overlapf(signature, context)\n",
    "        debug_trace(f\"{pp.info('INFO')}: {sense} scored: {pp.success(overlap)}\")\n",
    "        if overlap > max_overlap:\n",
    "            max_overlap = overlap\n",
    "            best_sense = sense\n",
    "    return best_sense"
   ]
  },
  {
   "source": [
    "Una versione leggermente modificata dell'algoritmo di Lesk, rispetto a quella originale:\n",
    "\n",
    "1. Aggiunge gli 'esempi' di WordNet come parte della signature (più parole ci sono, più dovrebbe essere facile disambiguare)\n",
    "2. Rimuove la parola target dalle definizioni (signature e contesto)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lesk_custom(word, sentence, overlapf=computeOverlap):\n",
    "    try:\n",
    "        best_sense = wn.synsets(word)[0]\n",
    "    except IndexError:\n",
    "        best_sense = None\n",
    "    max_overlap = 0\n",
    "    context = utils.word_frequencies(utils.preprocess_phrase(sentence))\n",
    "    for sense in wn.synsets(word):\n",
    "        debug_trace(f\"{pp.info('INFO')}: Considering new sense: {sense}({pp.bold(pp.info(sense.definition()))})\")\n",
    "        examples = sense.examples().copy()\n",
    "        examples.append(sense.definition())\n",
    "        debug_trace(f\"{pp.warning(pp.bold('EXAMPLES'))}: {examples}\")\n",
    "        signature = utils.word_frequencies(utils.preprocess_phrase(functools.reduce(operator.add, examples)))\n",
    "        overlap = overlapf(signature, context)\n",
    "        debug_trace(f\"{pp.info('INFO')}: {sense} scored: {pp.success(overlap)}\")\n",
    "        if overlap > max_overlap:\n",
    "            max_overlap = overlap\n",
    "            best_sense = sense\n",
    "    return best_sense"
   ]
  },
  {
   "source": [
    "Una versione leggermente modificata dell'algoritmo di Lesk, rispetto a quella originale\n",
    "\n",
    "1. Aggiunge gli 'esempi' di WordNet come parte della signature (più parole ci sono, più dovrebbe essere facile disambiguare)\n",
    "2. Aumenta le dimensioni del contesto per la signature: prende esempi e definizioni del senso, e dei i suoi iperonimi e iponimi diretti\n",
    "3. Lascia la parola target nella signature"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lesk_custom_bigcontext(word, sentence, overlapf=computeOverlap):\n",
    "    try:\n",
    "        best_sense = wn.synsets(word)[0]\n",
    "    except IndexError:\n",
    "        best_sense = None\n",
    "    max_overlap = 0\n",
    "    context = utils.word_frequencies(utils.preprocess_phrase(sentence))\n",
    "    for sense in wn.synsets(word):\n",
    "        debug_trace(f\"{pp.info('INFO')}: Considering new sense: {sense}({pp.bold(pp.info(sense.definition()))})\")\n",
    "        signature = get_big_disambiguation_ctx_for_sysnet(sense)\n",
    "        overlap = overlapf(signature, context)\n",
    "        debug_trace(f\"{pp.info('INFO')}: {sense} scored: {pp.success(overlap)}\")\n",
    "        if overlap > max_overlap:\n",
    "            max_overlap = overlap\n",
    "            best_sense = sense\n",
    "    return best_sense"
   ]
  },
  {
   "source": [
    "Una versione leggermente modificata dell'algoritmo di Lesk, rispetto a quella originale\n",
    "\n",
    "1. Aggiunge gli 'esempi' di WordNet come parte della signature (più parole ci sono, più dovrebbe essere facile disambiguare)\n",
    "2. Aumenta le dimensioni del contesto per la signature: prende esempi e definizioni del senso, e dei i suoi iperonimi e iponimi diretti\n",
    "3. Lascia la parola target nella signature\n",
    "4. Non fa stemming"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7446b797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lesk_custom_nostem(word, sentence, overlapf=computeOverlap):\n",
    "    try:\n",
    "        best_sense = wn.synsets(word)[0]\n",
    "    except IndexError:\n",
    "        best_sense = None\n",
    "    max_overlap = 0\n",
    "    context = utils.word_frequencies(utils.preprocess_phrase_nostem(sentence))\n",
    "    for sense in wn.synsets(word):\n",
    "        if sense.pos() == 'n':\n",
    "            debug_trace(f\"{pp.info('INFO')}: Considering new sense: {sense}({pp.bold(pp.info(sense.definition()))})\")\n",
    "            signature = get_big_disambiguation_ctx_for_sysnet(sense, do_stemming=False)\n",
    "            overlap = overlapf(signature, context)\n",
    "            debug_trace(f\"{pp.info('INFO')}: {sense} scored: {pp.success(overlap)}\")\n",
    "            if overlap > max_overlap:\n",
    "                max_overlap = overlap\n",
    "                best_sense = sense\n",
    "    return best_sense"
   ]
  },
  {
   "source": [
    "Nel blocco seguente, recupero le frasi da SemCor, avendo l'accortezza di recuperare entrambi i tags (semantico e PoS). Si potrebbero contare le frasi (usando il commento) ma ci vuole molto tempo, ho preferito inserirlo a mano."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sc.tagged_sents(tag='both')\n",
    "sents_len = 37176 # len(sentences)"
   ]
  },
  {
   "source": [
    "La prossima funzione prende una frase da SemCor (rappresentata come un albero), e la restituisce come stringa. (Prende tutte le foglie e le mette in una sola lista, successivamente, 'incolla' i singoli elementi separandoli con uno spazio)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phrase_from_sentence(semcor_sentence):\n",
    "    \"\"\"\n",
    "        Returns a (pseudo-)phrase from a semcor tagged sentence\n",
    "    \"\"\"\n",
    "    return functools.reduce(lambda x, y: x + \" \" + y, utils.flatten([x.leaves() for x in semcor_sentence]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "La seguente funzione, prende tutti i possibili alberi da una frase SemCor per recuperarne tutti i sostantivi (usando una depth-first search)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noun_tuples(trees):\n",
    "    result = []\n",
    "    \"\"\"\n",
    "        Depth-first search that gets (for the trees of a phrase)\n",
    "    \"\"\"\n",
    "    for subtree in trees:\n",
    "        if type(subtree) == nltk.tree.Tree:\n",
    "            if type(subtree.label()) == nltk.corpus.reader.wordnet.Lemma:\n",
    "                if (subtree.label().synset().pos()) == 'n':\n",
    "                    word = list(subtree.subtrees())[1][0]\n",
    "                    if type(word) != nltk.tree.Tree:\n",
    "                        result.append((word, subtree.label()))\n",
    "            get_noun_tuples(subtree)\n",
    "    return result"
   ]
  },
  {
   "source": [
    "La prossima funzione recupera una lista contenente n numeri randomici. Il parametro `unique` specifica se il risultato può contenere o meno ripetizioni."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_random_numbers(n, max, unique=True):\n",
    "    result = []\n",
    "    if unique:\n",
    "        for i in range(n):\n",
    "            r = random.randint(0, max)\n",
    "            while r in result:\n",
    "                r = random.randint(0, max)\n",
    "            result.append(r)\n",
    "        return result\n",
    "    else:\n",
    "        return [random.randint(0, max) for i in range(0, n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "La prossima funzione prende una lista di indici e seleziona le frasi di SemCor corrispondenti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_phrases_from_indexes(indexes):\n",
    "    result = None\n",
    "    try:\n",
    "        result = [sentences[i] for i in indexes]\n",
    "    except NameError as e:\n",
    "        sentences = sc.tagged_sents(tag='both')\n",
    "        result = [sentences[i] for i in indexes]\n",
    "    return result"
   ]
  },
  {
   "source": [
    "### Task \\#1: Test drive\n",
    " * Estrarre 50 frasi da SemCor\n",
    " * Prova a disambiguare almeno un sostantivo per frase\n",
    " * Calcola l'accuratezza"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(leskf=lesk):\n",
    "    indexes = get_n_random_numbers(50, sents_len)\n",
    "    tagged_sentences = [sentences[i] for i in indexes]\n",
    "    fifty_sent = [get_phrase_from_sentence(i) for i in tagged_sentences]\n",
    "    accuracy = 0\n",
    "    base_accuracy = 0\n",
    "    for i in range(50):\n",
    "        targets = get_noun_tuples(tagged_sentences[i])\n",
    "        if len(targets) > 0:\n",
    "            nouns, correct = zip(*targets)\n",
    "            selected_noun = random.randint(0, len(nouns)-1)\n",
    "            debug = False\n",
    "            res = leskf(nouns[selected_noun], fifty_sent[i])\n",
    "            baseres = nltk.wsd.lesk(fifty_sent[i].split(' '), nouns[selected_noun], 'n')\n",
    "            correct_ss = correct[selected_noun].synset()\n",
    "            debug = True\n",
    "            debug_trace(f\"Iteration: {i}\")\n",
    "            debug_trace(f\"{pp.info('Noun')}: {nouns[selected_noun]} | {pp.info('Phrase')}: {fifty_sent[i]}\")\n",
    "            debug_trace(f\"{pp.info('Lesk Result')}: {res}\")\n",
    "            debug_trace(f\"{pp.info('Base Lesk Result')}: {baseres}\")\n",
    "            debug_trace(f\"{pp.info('Correct result')}: {correct_ss}\")\n",
    "            debug_trace(\"\")\n",
    "            try:\n",
    "                accuracy += int(res == correct_ss)/50\n",
    "                base_accuracy += int(baseres == correct_ss)/50\n",
    "            except AttributeError as e:\n",
    "                print(e)\n",
    "                pass\n",
    "    print(f\"{pp.bold(pp.success('Base Lesk Accuracy'))}: {base_accuracy*100}%\")\n",
    "    print(f\"{pp.bold(pp.success('Accuracy'))}: {accuracy*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[1m\u001b[92mBase Lesk Accuracy\u001b[0m\u001b[0m: 4.0%\n\u001b[1m\u001b[92mAccuracy\u001b[0m\u001b[0m: 14.000000000000002%\n"
     ]
    }
   ],
   "source": [
    "debug = False\n",
    "evaluate_accuracy()"
   ]
  },
  {
   "source": [
    "### Task \\#2: Test con randomizzazione\n",
    " * Estrarre 50 frasi da SemCor\n",
    " * Provare a disambiguare almeno un sostantivo per frase\n",
    " * Calcolare l'accuratezza\n",
    " * Calcolare l'accuratezza media su _n_ esecuzioni"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_evaluate(max_runs=50):\n",
    "    for _ in range(max_runs):\n",
    "        indexes = get_n_random_numbers(50, sents_len-1)\n",
    "        tagged_sentences = [sentences[i] for i in indexes]\n",
    "        fifty_sent = [get_phrase_from_sentence(i) for i in tagged_sentences]\n",
    "\n",
    "        lesk_baseline_accuracies = []\n",
    "        lesk_accuracies = []\n",
    "        lesk_custom_accuracies = []\n",
    "        lesk_custom_bigcontext_accuracies = []\n",
    "        lesk_custom_nostem_accuracies = []\n",
    "\n",
    "        lesk_baseline_accuracy = 0\n",
    "        lesk_accuracy = 0\n",
    "        lesk_custom_accuracy = 0\n",
    "        lesk_custom_bigcontext_accuracy = 0\n",
    "        lesk_custom_nostem_accuracy = 0\n",
    "        for i in range(49):\n",
    "            targets = get_noun_tuples(tagged_sentences[i])\n",
    "            if len(targets) > 0:\n",
    "                nouns, correct = zip(*targets)\n",
    "                selected_noun = random.randint(0, len(nouns)-1)\n",
    "                res_lesk = lesk(nouns[selected_noun], fifty_sent[i])\n",
    "                res_lesk_custom = lesk_custom(nouns[selected_noun], fifty_sent[i])\n",
    "                res_lesk_custom = lesk_custom_bigcontext(nouns[selected_noun], fifty_sent[i])\n",
    "                res_lesk_custom_nostem = lesk_custom_nostem(nouns[selected_noun], fifty_sent[i])\n",
    "                correct_ss = correct[selected_noun].synset()\n",
    "                try:\n",
    "                    lesk_baseline_accuracy += int(res_lesk == correct_ss)/50\n",
    "                    lesk_accuracy += int(res_lesk == correct_ss)/50\n",
    "                    lesk_custom_accuracy += int(res_lesk_custom == correct_ss)/50\n",
    "                    lesk_custom_bigcontext_accuracy += int(res_lesk_custom == correct_ss)/50\n",
    "                    lesk_custom_nostem_accuracy += int(res_lesk_custom_nostem == correct_ss)/50\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "        lesk_baseline_accuracies.append(lesk_baseline_accuracy)\n",
    "        lesk_accuracies.append(lesk_accuracy)\n",
    "        lesk_custom_accuracies.append(lesk_custom_accuracy)\n",
    "        lesk_custom_bigcontext_accuracies.append(lesk_custom_bigcontext_accuracy)\n",
    "        lesk_custom_nostem_accuracies.append(lesk_custom_nostem_accuracy)\n",
    "    print(f\"{pp.bold(pp.success('NLTK Lesk Mean Accuracy'))}: {np.mean(lesk_baseline_accuracies)*100}%\")\n",
    "    print(f\"{pp.bold(pp.success('Lesk Mean Accuracy'))}: {np.mean(lesk_accuracies)*100}%\")\n",
    "    print(f\"{pp.bold(pp.success('Custom Lesk Mean Accuracy'))}: {np.mean(lesk_custom_accuracies)*100}%\")\n",
    "    print(f\"{pp.bold(pp.success('BigContext Custom Lesk Mean Accuracy'))}: {np.mean(lesk_custom_bigcontext_accuracies)*100}%\")\n",
    "    print(f\"{pp.bold(pp.success('Nostem Custom Lesk Mean Accuracy'))}: {np.mean(lesk_custom_nostem_accuracies)*100}%\")\n",
    "    return lesk_accuracies, lesk_custom_accuracies, lesk_custom_bigcontext_accuracies, lesk_custom_nostem_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iterations: 50\n",
      "\u001b[1m\u001b[92mNLTK Lesk Mean Accuracy\u001b[0m\u001b[0m: 25.999999999999996%\n",
      "\u001b[1m\u001b[92mLesk Mean Accuracy\u001b[0m\u001b[0m: 25.999999999999996%\n",
      "\u001b[1m\u001b[92mCustom Lesk Mean Accuracy\u001b[0m\u001b[0m: 36.00000000000001%\n",
      "\u001b[1m\u001b[92mBigContext Custom Lesk Mean Accuracy\u001b[0m\u001b[0m: 36.00000000000001%\n",
      "\u001b[1m\u001b[92mNostem Custom Lesk Mean Accuracy\u001b[0m\u001b[0m: 40.00000000000001%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "debug=False\n",
    "print(\"Iterations: 50\")\n",
    "round_evaluate(max_runs=50)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iterations: 100\n",
      "\u001b[1m\u001b[92mNLTK Lesk Mean Accuracy\u001b[0m\u001b[0m: 23.999999999999996%\n",
      "\u001b[1m\u001b[92mLesk Mean Accuracy\u001b[0m\u001b[0m: 23.999999999999996%\n",
      "\u001b[1m\u001b[92mCustom Lesk Mean Accuracy\u001b[0m\u001b[0m: 21.999999999999996%\n",
      "\u001b[1m\u001b[92mBigContext Custom Lesk Mean Accuracy\u001b[0m\u001b[0m: 21.999999999999996%\n",
      "\u001b[1m\u001b[92mNostem Custom Lesk Mean Accuracy\u001b[0m\u001b[0m: 30.0%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Iterations: 100\")\n",
    "round_evaluate(max_runs=100)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iterations: 1000\n",
      "\u001b[1m\u001b[92mNLTK Lesk Mean Accuracy\u001b[0m\u001b[0m: 21.999999999999996%\n",
      "\u001b[1m\u001b[92mLesk Mean Accuracy\u001b[0m\u001b[0m: 21.999999999999996%\n",
      "\u001b[1m\u001b[92mCustom Lesk Mean Accuracy\u001b[0m\u001b[0m: 25.999999999999996%\n",
      "\u001b[1m\u001b[92mBigContext Custom Lesk Mean Accuracy\u001b[0m\u001b[0m: 25.999999999999996%\n",
      "\u001b[1m\u001b[92mNostem Custom Lesk Mean Accuracy\u001b[0m\u001b[0m: 34.0%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Iterations: 1000\")\n",
    "round_evaluate(max_runs=1000)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python39664bitf9365f4bd6c94cd0b30113d0555f72e7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}